{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQyTEgCiMlQE"
   },
   "source": [
    "# Assignment 1: Decision Trees and Nearest Neighbor Classification\n",
    " \n",
    "In this assignment you will explore the use of decision trees and nearest neighbor classifiers to learn morphological classes. You will make use of standard implementations of these classifiers in scikit learn.\n",
    " \n",
    "Provided are two word morphology data sets describing the formation of inflected word forms from lemma information for the plural forms of German nouns (german_plural.arff) and the past tense forms of English verbs (english_past_tense.arff), respectively. You will evaluate two classifiers, decision trees and k nearest neighbours, for predicting the class from test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWkYzx0vvsTA"
   },
   "source": [
    "The data is in the arff format, which is the standard format of the machine learning package Weka, and more or less self-explanatory. If you want to know more about the format, consult the Weka documentation (https://www.cs.waikato.ac.nz/ml/weka/).\n",
    "\n",
    "The files can be downloaded from the student portal or from the given urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_wdMb5IT6X_"
   },
   "outputs": [],
   "source": [
    "data_urls = [\"\"\"https://cl.lingfil.uu.se/~frewa417/english_past_tense.arff\"\"\", \"\"\"https://cl.lingfil.uu.se/~frewa417/german_plural.arff\"\"\"]\n",
    "\n",
    "filenames = [url.split(\"/\")[-1] for url in data_urls]\n",
    "\n",
    "import urllib.request\n",
    "for url, fn in zip(data_urls, filenames):\n",
    "  urllib.request.urlretrieve(url, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zaHHAcHfNSfa"
   },
   "source": [
    "The two data files should now be downloaded. We can see if this is the case by running a shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TzOHRNRVNR9b",
    "outputId": "b55448a1-0af5-4fa9-cb01-0be16500f271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_past_tense.arff  german_plural.arff  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6wUjcnONYQ0"
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "Your first task is to get acquainted with the data sets.\n",
    "\n",
    "\n",
    "The first data set contains 25168 instances, each of which is a German noun lemma, and that each instance has 9 attributes, of which the class attributes encode the plural formation of the noun and is the attribute that we want to learn to predict. The attribute **frequency** records corpus frequency, the attribute **gender** represents the gender of the noun, and the attributes **p1-p6** give a phonological representation of the last two syllables of the base form. Specifically, p1, p2 and p3 represent the onset, nucleus and coda of the penultimate syllable, and p4, p5 and p6 those of the ultimate syllable. By selecting different attributes, you can inspect their type, value set and distribution in the data set.\n",
    "\n",
    "Next the *English past tense data set*, which consists of 4330 verb lemmas and where the class to predict is the past tense formation rule. Attributes are similar to the ones found in the German data set, except that the phonological representation covers the last three syllables (p1-p3: antepenultimate, p4-p6: penultimate, p7-p9: ultimate) and that there is no gender attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfseBuawPdQj"
   },
   "source": [
    "The data files are in an uncompressed text format. The first part describes the data (note that all features are categorical) and the second part is the data points. We can look at the first 20 lines of both files using the **head** shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "bnoLLZSrJlTm",
    "outputId": "0bf08d8c-b0b6-4114-a476-cea049d3d428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@relation plural\n",
      "@attribute frequency NUMERIC\n",
      "@attribute p1 {+, +l, +r, -, =, =l, =v, J, N, S, Sl, Sm, Sn, Sp, Spl, Spr, Sr, St, Str, Sv, Z, _, b, bl, br, d, dl, dm, dn, dr, f, fl, fr, g, gl, gm, gn, gr, h, j, k, kl, kn, kr, ks, kv, l, m, n, p, pj, pl, pn, pr, ps, pt, r, s, s=, sf, sk, skl, skr, sl, sm, sp, st, stj, str, sv, t, tl, tn, tr, v, vj, x, z}\n",
      "@attribute p2 {#, &, ), -, /, 0, 1, 2, 6, @, B, E, I, O, U, W, X, Y, a, e, i, o, q, u, y, |, ~}\n",
      "@attribute p3 {+, -, =, =t, J, N, Nk, Nkt, Ns, Nst, Nt, S, d, f, f=, fs, ft, k, ks, kst, kt, l, l=, lS, lf, lfs, lk, lks, lm, lp, lps, lpst, ls, lst, lt, lx, m, m+, m=, mf, mp, ms, mt, n, n=, nJ, nS, nd, nf, nf=, nft, ns, nst, nt, ntl, nx, p, ps, pst, pt, r, r=, rS, rf, rk, rkt, rl, rm, rn, rnst, rp, rpst, rs, rst, rt, rx, s, sp, st, t, x, x=, xs, xst, xt}\n",
      "@attribute p4 {+, +l, +r, -, =, =l, =v, J, N, Nl, S, Sl, Sm, Sn, Sp, Spl, Spr, Sr, St, Str, Sv, Z, _, _m, b, bl, br, d, dl, dn, dr, f, fj, fl, fr, ft, g, gl, gm, gn, gr, h, j, k, kl, kn, kr, ks, kv, l, m, ml, n, p, pl, pn, pr, ps, r, s, s=, sf, sk, skr, skv, sl, sn, sp, sr, st, str, sv, t, tl, tn, tr, tv, v, vj, vr, x, xl, z, zn} \n",
      "@attribute p5 {#, $, &, ), /, 0, 4, @, B, E, I, O, U, W, X, Y, ^, a, e, i, o, q, u, y, (, |, ~}\n",
      "@attribute p6 {+, -, =, J, N, Nk, Nks, Nkt, Nst, S, Z, _, b, f, ft, k, ks, kst, kt, l, l=, lf, lk, lm, lp, lps, ls, lst, lt, lx, m, m+, mS, mf, mp, mps, ms, mt, n, n=, nJ, nS, nd, nf, nft, ns, nst, nt, nx, p, ps, pst, pt, r, r=, r=t, rS, rf, rft, rk, rkt, rl, rm, rn, rp, rpst, rpt, rs, rst, rt, rx, s, sk, st, t, v, x, x=, xt}\n",
      "@attribute gender {-, F, FM, FN, M, MF, MN, N, NF, NM}\n",
      "@attribute class {-, U, Ue, Uer, e, en, er, s}\n",
      "\n",
      "@data\n",
      "11,l,a,-,d,U,N,F,en\n",
      "2,k,/,-,x,@,r,M,-\n",
      "10,t,o,n,-,a,rt,F,en\n",
      "11,h,E,N,z,@,l,N,-\n",
      "2,-,-,-,kn,&,st,M,e\n",
      "0,-,e,-,r,@,-,F,en\n",
      "0,t,e,-,br,a,t,M,en\n",
      "59,g,@,-,v,I,xt,N,e\n"
     ]
    }
   ],
   "source": [
    "!head -n20 german_plural.arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "e2TJ_yc_JOWQ",
    "outputId": "c1709525-091c-4ed7-9854-5c2ef033326e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@relation past-tense\n",
      "@attribute frequency NUMERIC\n",
      "@attribute p1 {=, @, &, 3:, A:, aI, aU, b, bj, bl, bl#, br, d, D, dj, dl#, dn#, dr, dw, dZ, E, E@, eI, f, fj, fl, fl#, fn#, fr, ft, g, gj, gl, gl#, gr, gw, h, hj, i:, I, I@, j, k, kj, kl, kl#, kr, ks, kt, kw, l, ld, ldZ, lf, lj, lk, lm, lp, lpt, ls, lS, lt, ltS, lv, m, mf, mj, ml#, mp, mps, mpt, n, N, nd, ndZ, nj, Nk, nl#, ns, nt, ntS, nz, O, O:, OI, p, pj, pl, pl#, pr, ps, pt, r, r*, s, S, sj, sk, skj, skl, skr, skw, sl, sl#, Sl#, sm, sn, sn#, Sn#, sp, spj, spl, spr, Sr, st, stj, str, sw, t, T, tj, Tj, tl#, tn#, Tn#, tr, Tr, ts, tS, tSr, tw, Tw, u:, U, @U, U@, v, V, vj, vl#, vn#, vz, w, z, Z, zj, zl#, zn#, Zn#}\n",
      "@attribute p2 {=, @, &, 3:, A:, aI, aU, b, bj, bl, bl#, br, d, D, dj, dl#, dn#, dr, dw, dZ, E, E@, eI, f, fj, fl, fl#, fn#, fr, ft, g, gj, gl, gl#, gr, gw, h, hj, i:, I, I@, j, k, kj, kl, kl#, kr, ks, kt, kw, l, ld, ldZ, lf, lj, lk, lm, lp, lpt, ls, lS, lt, ltS, lv, m, mf, mj, ml#, mp, mps, mpt, n, N, nd, ndZ, nj, Nk, nl#, ns, nt, ntS, nz, O, O:, OI, p, pj, pl, pl#, pr, ps, pt, r, r*, s, S, sj, sk, skj, skl, skr, skw, sl, sl#, Sl#, sm, sn, sn#, Sn#, sp, spj, spl, spr, Sr, st, stj, str, sw, t, T, tj, Tj, tl#, tn#, Tn#, tr, Tr, ts, tS, tSr, tw, Tw, u:, U, @U, U@, v, V, vj, vl#, vn#, vz, w, z, Z, zj, zl#, zn#, Zn#}\n",
      "@attribute p3 {=, @, &, 3:, A:, aI, aU, b, bj, bl, bl#, br, d, D, dj, dl#, dn#, dr, dw, dZ, E, E@, eI, f, fj, fl, fl#, fn#, fr, ft, g, gj, gl, gl#, gr, gw, h, hj, i:, I, I@, j, k, kj, kl, kl#, kr, ks, kt, kw, l, ld, ldZ, lf, lj, lk, lm, lp, lpt, ls, lS, lt, ltS, lv, m, mf, mj, ml#, mp, mps, mpt, n, N, nd, ndZ, nj, Nk, nl#, ns, nt, ntS, nz, O, O:, OI, p, pj, pl, pl#, pr, ps, pt, r, r*, s, S, sj, sk, skj, skl, skr, skw, sl, sl#, Sl#, sm, sn, sn#, Sn#, sp, spj, spl, spr, Sr, st, stj, str, sw, t, T, tj, Tj, tl#, tn#, Tn#, tr, Tr, ts, tS, tSr, tw, Tw, u:, U, @U, U@, v, V, vj, vl#, vn#, vz, w, z, Z, zj, zl#, zn#, Zn#}\n",
      "@attribute p4 {=, @, &, 3:, A:, aI, aU, b, bj, bl, bl#, br, d, D, dj, dl#, dn#, dr, dw, dZ, E, E@, eI, f, fj, fl, fl#, fn#, fr, ft, g, gj, gl, gl#, gr, gw, h, hj, i:, I, I@, j, k, kj, kl, kl#, kr, ks, kt, kw, l, ld, ldZ, lf, lj, lk, lm, lp, lpt, ls, lS, lt, ltS, lv, m, mf, mj, ml#, mp, mps, mpt, n, N, nd, ndZ, nj, Nk, nl#, ns, nt, ntS, nz, O, O:, OI, p, pj, pl, pl#, pr, ps, pt, r, r*, s, S, sj, sk, skj, skl, skr, skw, sl, sl#, Sl#, sm, sn, sn#, Sn#, sp, spj, spl, spr, Sr, st, stj, str, sw, t, T, tj, Tj, tl#, tn#, Tn#, tr, Tr, ts, tS, tSr, tw, Tw, u:, U, @U, U@, v, V, vj, vl#, vn#, vz, w, z, Z, zj, zl#, zn#, Zn#}\n",
      "@attribute p5 {=, @, &, 3:, A:, aI, aU, b, bj, bl, bl#, br, d, D, dj, dl#, dn#, dr, dw, dZ, E, E@, eI, f, fj, fl, fl#, fn#, fr, ft, g, gj, gl, gl#, gr, gw, h, hj, i:, I, I@, j, k, kj, kl, kl#, kr, ks, kt, kw, l, ld, ldZ, lf, lj, lk, lm, lp, lpt, ls, lS, lt, ltS, lv, m, mf, mj, ml#, mp, mps, mpt, n, N, nd, ndZ, nj, Nk, nl#, ns, nt, ntS, nz, O, O:, OI, p, pj, pl, pl#, pr, ps, pt, r, r*, s, S, sj, sk, skj, skl, skr, skw, sl, sl#, Sl#, sm, sn, sn#, Sn#, sp, spj, spl, spr, Sr, st, stj, str, sw, t, T, tj, Tj, tl#, tn#, Tn#, tr, Tr, ts, tS, tSr, tw, Tw, u:, U, @U, U@, v, V, vj, vl#, vn#, vz, w, z, Z, zj, zl#, zn#, Zn#}\n",
      "@attribute p6 {=, @, &, 3:, A:, aI, aU, b, bj, bl, bl#, br, d, D, dj, dl#, dn#, dr, dw, dZ, E, E@, eI, f, fj, fl, fl#, fn#, fr, ft, g, gj, gl, gl#, gr, gw, h, hj, i:, I, I@, j, k, kj, kl, kl#, kr, ks, kt, kw, l, ld, ldZ, lf, lj, lk, lm, lp, lpt, ls, lS, lt, ltS, lv, m, mf, mj, ml#, mp, mps, mpt, n, N, nd, ndZ, nj, Nk, nl#, ns, nt, ntS, nz, O, O:, OI, p, pj, pl, pl#, pr, ps, pt, r, r*, s, S, sj, sk, skj, skl, skr, skw, sl, sl#, Sl#, sm, sn, sn#, Sn#, sp, spj, spl, spr, Sr, st, stj, str, sw, t, T, tj, Tj, tl#, tn#, Tn#, tr, Tr, ts, tS, tSr, tw, Tw, u:, U, @U, U@, v, V, vj, vl#, vn#, vz, w, z, Z, zj, zl#, zn#, Zn#}\n",
      "@attribute p7 {=, @, &, 3:, A:, aI, aU, b, bj, bl, bl#, br, d, D, dj, dl#, dn#, dr, dw, dZ, E, E@, eI, f, fj, fl, fl#, fn#, fr, ft, g, gj, gl, gl#, gr, gw, h, hj, i:, I, I@, j, k, kj, kl, kl#, kr, ks, kt, kw, l, ld, ldZ, lf, lj, lk, lm, lp, lpt, ls, lS, lt, ltS, lv, m, mf, mj, ml#, mp, mps, mpt, n, N, nd, ndZ, nj, Nk, nl#, ns, nt, ntS, nz, O, O:, OI, p, pj, pl, pl#, pr, ps, pt, r, r*, s, S, sj, sk, skj, skl, skr, skw, sl, sl#, Sl#, sm, sn, sn#, Sn#, sp, spj, spl, spr, Sr, st, stj, str, sw, t, T, tj, Tj, tl#, tn#, Tn#, tr, Tr, ts, tS, tSr, tw, Tw, u:, U, @U, U@, v, V, vj, vl#, vn#, vz, w, z, Z, zj, zl#, zn#, Zn#}\n",
      "@attribute p8 {=, @, &, 3:, A:, aI, aU, b, bj, bl, bl#, br, d, D, dj, dl#, dn#, dr, dw, dZ, E, E@, eI, f, fj, fl, fl#, fn#, fr, ft, g, gj, gl, gl#, gr, gw, h, hj, i:, I, I@, j, k, kj, kl, kl#, kr, ks, kt, kw, l, ld, ldZ, lf, lj, lk, lm, lp, lpt, ls, lS, lt, ltS, lv, m, mf, mj, ml#, mp, mps, mpt, n, N, nd, ndZ, nj, Nk, nl#, ns, nt, ntS, nz, O, O:, OI, p, pj, pl, pl#, pr, ps, pt, r, r*, s, S, sj, sk, skj, skl, skr, skw, sl, sl#, Sl#, sm, sn, sn#, Sn#, sp, spj, spl, spr, Sr, st, stj, str, sw, t, T, tj, Tj, tl#, tn#, Tn#, tr, Tr, ts, tS, tSr, tw, Tw, u:, U, @U, U@, v, V, vj, vl#, vn#, vz, w, z, Z, zj, zl#, zn#, Zn#}\n",
      "@attribute p9 {=, @, &, 3:, A:, aI, aU, b, bj, bl, bl#, br, d, D, dj, dl#, dn#, dr, dw, dZ, E, E@, eI, f, fj, fl, fl#, fn#, fr, ft, g, gj, gl, gl#, gr, gw, h, hj, i:, I, I@, j, k, kj, kl, kl#, kr, ks, kt, kw, l, ld, ldZ, lf, lj, lk, lm, lp, lpt, ls, lS, lt, ltS, lv, m, mf, mj, ml#, mp, mps, mpt, n, N, nd, ndZ, nj, Nk, nl#, ns, nt, ntS, nz, O, O:, OI, p, pj, pl, pl#, pr, ps, pt, r, r*, s, S, sj, sk, skj, skl, skr, skw, sl, sl#, Sl#, sm, sn, sn#, Sn#, sp, spj, spl, spr, Sr, st, stj, str, sw, t, T, tj, Tj, tl#, tn#, Tn#, tr, Tr, ts, tS, tSr, tw, Tw, u:, U, @U, U@, v, V, vj, vl#, vn#, vz, w, z, Z, zj, zl#, zn#, Zn#}\n",
      "@attribute class { IRREG-aI-aU.,  IRREG-aI-eI.,  IRREG-aI-I.,  IRREG-aI-O:., IRREG-aI-O.,  IRREG-aI-u:.,  IRREG-aI-@U.,  IRREG-aI-V.,  IRREG-CONV., IRREG-@+d.,  IRREG-d-t.,  IRREG-E-&.,  IRREG-eI-u:.,  IRREG-eI-@U., IRREG-eI-U.,  IRREG-E-O.,  IRREG-E@-O:.,  IRREG-I-&.,  IRREG-i:-E., IRREG-i:-E+d.,  IRREG-I-eI.,  IRREG-i:-E+t.,  IRREG-i:-O:., IRREG-i:-@U.,  IRREG-I-U.,  IRREG-I-V.,  IRREG-@+ked.,  IRREG-O:-E., IRREG-OTHER.,  IRREG-O-u.,  IRREG-O:-u:.,  IRREG-@-s+ed.,  IRREG-@+t., IRREG-@U-E.,  IRREG-@U-ju:.,  IRREG-u:-O.,  IRREG-u:-@U., IRREG-@U-u:.,  IRREG-&-V.,  IRREG-V-&.,  IRREG-V-eI.,  REG.}\n",
      "\n",
      "@data\n",
      "975,=,@,=,b,&,n,d,@,n,REG.\n",
      "15,=,=,=,=,@,=,b,&,S,REG.\n",
      "42,=,=,=,=,@,=,b,eI,t,REG.\n",
      "35,br,i:,=,v,I,=,=,eI,t,REG.\n",
      "40,=,&,b,d,I,=,k,eI,t,REG.\n",
      "19,=,=,=,=,@,b,d,V,kt,REG.\n"
     ]
    }
   ],
   "source": [
    "!head -n20 english_past_tense.arff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcwuQ-NZvwuP"
   },
   "source": [
    "Loading the data files can be done using scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5QYHyObVZ6P"
   },
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "loaded_data_files = [loadarff(fn) for fn in filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gxv9Mm2a_TTV"
   },
   "source": [
    "Now you have the data in memory. However, it needs to be re-formated to fit sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHQygK9YuRJ0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = dict()\n",
    "for data in loaded_data_files:\n",
    "  data_points = data[0]\n",
    "  field_names = data[1].names()\n",
    "  assert field_names[0] == 'frequency'\n",
    "  assert field_names[-1] == 'class'\n",
    "  X = list()\n",
    "  y = list()\n",
    "  for point in data_points:\n",
    "    v = [field_names[i]+\"_\"+point[i].decode(\"utf-8\") for i in range(1, len(point)-1)]\n",
    "    X.extend([v]*int(point[0]))\n",
    "    assert len(v) == len(X[0])\n",
    "    u = [point[-1].decode(\"utf-8\")]\n",
    "    y.extend([u]*int(point[0]))\n",
    "    assert len(u) == len(y[0])\n",
    "  assert len(X) == np.sum(np.asarray([point[0] for point in data_points]))\n",
    "  X_orig = np.asarray(X)\n",
    "  y_orig = np.asarray(y).ravel()\n",
    "  D[data[1].name] = tuple([X_orig, y_orig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IyhNXsIfRAr1"
   },
   "source": [
    "The two data sets are now contained in a dictionary with the following keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6jiLJ4nnOGZ5",
    "outputId": "c1dfcf26-1b34-40ce-99d4-8c85c2a4d4fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['past-tense', 'plural'])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LS3PXzL3RrBl"
   },
   "source": [
    "Each entry in the dictionary is a tuple (X, y) with categorical features. Note that the features are not yet in a numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "P9d5Qg1mQj1e",
    "outputId": "d060f7ea-11de-4a3e-da31-58eda95ea135"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['p1_=', 'p2_@', 'p3_=', ..., 'p7_d', 'p8_@', 'p9_n'],\n",
       "        ['p1_=', 'p2_@', 'p3_=', ..., 'p7_d', 'p8_@', 'p9_n'],\n",
       "        ['p1_=', 'p2_@', 'p3_=', ..., 'p7_d', 'p8_@', 'p9_n'],\n",
       "        ...,\n",
       "        ['p1_=', 'p2_=', 'p3_=', ..., 'p7_z', 'p8_u:', 'p9_m'],\n",
       "        ['p1_=', 'p2_=', 'p3_=', ..., 'p7_z', 'p8_u:', 'p9_m'],\n",
       "        ['p1_=', 'p2_=', 'p3_=', ..., 'p7_z', 'p8_u:', 'p9_m']],\n",
       "       dtype='<U6'),\n",
       " array(['REG.', 'REG.', 'REG.', ..., 'REG.', 'REG.', 'REG.'], dtype='<U13'))"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D['past-tense']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "fEmDnavnQSlo",
    "outputId": "555ed7ff-0e59-410f-d998-28f38a42eb89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['p1_l', 'p2_a', 'p3_-', ..., 'p5_U', 'p6_N', 'gender_F'],\n",
       "        ['p1_l', 'p2_a', 'p3_-', ..., 'p5_U', 'p6_N', 'gender_F'],\n",
       "        ['p1_l', 'p2_a', 'p3_-', ..., 'p5_U', 'p6_N', 'gender_F'],\n",
       "        ...,\n",
       "        ['p1_l', 'p2_e', 'p3_-', ..., 'p5_@', 'p6_n', 'gender_N'],\n",
       "        ['p1_l', 'p2_e', 'p3_-', ..., 'p5_@', 'p6_n', 'gender_N'],\n",
       "        ['p1_l', 'p2_e', 'p3_-', ..., 'p5_@', 'p6_n', 'gender_N']],\n",
       "       dtype='<U9'),\n",
       " array(['en', 'en', 'en', ..., '-', '-', '-'], dtype='<U3'))"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D['plural']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_YvvL3VPMoh"
   },
   "source": [
    "## Making a space using categorical features\n",
    "\n",
    "Your first task is to preprocess the data to be able to analyse it with sklearn. The categorical features need to be converted into proper vectors. This can be done using some preprocessing tools from sklearn. The OneHotEncoder will transform a multiple output category into a set of binary feature dimensions. Using this might make the preprocessed feature vectors very long.\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "feature_encoder = OneHotEncoder()\n",
    "X = feature_encoder.fit_transform(X_orig)\n",
    "names = feature_encoder.get_feature_names()\n",
    "```\n",
    "\n",
    "Doing a similar encoding for the output data can be done using the LabelEncoder. This encodes the output class labels as numbers. Since we're only interested in the classification accuracy, there is no need for long binary vectors here.\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_orig)\n",
    "```\n",
    "\n",
    "Since this is the first lab in this course, you will be given snippets of code to help you along. To have some data to work with, the wine dataset from sklearn will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gbVXXho5svKf",
    "outputId": "95c4d063-f4b2-4a9b-9210-683500c052aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (178, 13) , y: (178,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "print(\"X:\", X.shape, \", y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grg-De0nSbDV"
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "The second task is to induce decision trees for predicting the plural form of a German noun and the past tense form of an English verb. Below, you are given code for how this works for the wine data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "iMdKazNvESDV",
    "outputId": "9e98944b-e6a7-40e0-d0ad-7e462888d392"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(max_depth=2)\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmrcyKroCBC1"
   },
   "source": [
    "Build decision trees for both language data sets and analyze their performance.\n",
    "\n",
    "Compare training error to test error and see whether there are signs of overfitting. Also, try out some different choices of hyper parameters (especially for [pruning](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning)) and see how this affects the size of the tree as well as the relation between training and test error. Splitting up the data can be done using the following examples. k-fold cross validation splits up the data and allows you to iterate over different training and test set to mitigate the effect of getting a \"well chosen\" training set by chance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "2jDm9kSyCBf9",
    "outputId": "5ddebe97-0f36-4c30-8909-6774c176bf9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.1%\n",
      "Accuracy: 91.7%\n",
      "Accuracy: 88.9%\n",
      "Accuracy: 82.9%\n",
      "Accuracy: 85.7%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "for train, test in KFold(n_splits=5, shuffle=True).split(X):\n",
    "  X_train = X[train]\n",
    "  X_test = X[test]\n",
    "  y_train = y[train]\n",
    "  y_test = y[test]\n",
    "  classifier.fit(X_train, y_train)\n",
    "  print(\"Accuracy: %.1f%%\" %(100*classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "DJCwBMQOCsOO",
    "outputId": "afa781dc-c023-474a-ad7e-0c14c7da8ec0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  1,  0],\n",
       "       [ 2,  7,  0],\n",
       "       [ 0,  0, 13]])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eacXGLgF-pCi"
   },
   "source": [
    "\n",
    "One of the advantages of decision trees, compared to many other learning algorithms, is that the induced classifier can be interpreted as a set of rules for classifying new instances. Visualising these rules is easy using graphviz (this can also be done using only sklearn but not with the same beautiful result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "T9bQe4UQ2D1-",
    "outputId": "c68fbc94-f00f-4a94-ede6-bfc565c8fbbe"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"554pt\" height=\"314pt\"\n",
       " viewBox=\"0.00 0.00 554.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-310 550,-310 550,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#dcfae9\" stroke=\"#000000\" d=\"M340.5,-306C340.5,-306 202.5,-306 202.5,-306 196.5,-306 190.5,-300 190.5,-294 190.5,-294 190.5,-235 190.5,-235 190.5,-229 196.5,-223 202.5,-223 202.5,-223 340.5,-223 340.5,-223 346.5,-223 352.5,-229 352.5,-235 352.5,-235 352.5,-294 352.5,-294 352.5,-300 346.5,-306 340.5,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">color_intensity &lt;= 3.82</text>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.648</text>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 142</text>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [45, 62, 35]</text>\n",
       "<text text-anchor=\"middle\" x=\"271.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = class_1</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#40e686\" stroke=\"#000000\" d=\"M249,-187C249,-187 144,-187 144,-187 138,-187 132,-181 132,-175 132,-175 132,-116 132,-116 132,-110 138,-104 144,-104 144,-104 249,-104 249,-104 255,-104 261,-110 261,-116 261,-116 261,-175 261,-175 261,-181 255,-187 249,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">proline &lt;= 1002.5</text>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.07</text>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 55</text>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 53, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = class_1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M245.2686,-222.8796C239.7627,-214.1434 233.8994,-204.8404 228.2176,-195.8253\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"231.1363,-193.8919 222.8433,-187.2981 225.2143,-197.6242 231.1363,-193.8919\"/>\n",
       "<text text-anchor=\"middle\" x=\"217.3137\" y=\"-207.9789\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#fbece1\" stroke=\"#000000\" d=\"M402,-187C402,-187 291,-187 291,-187 285,-187 279,-181 279,-175 279,-175 279,-116 279,-116 279,-110 285,-104 291,-104 291,-104 402,-104 402,-104 408,-104 414,-110 414,-116 414,-116 414,-175 414,-175 414,-181 408,-187 402,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"346.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">flavanoids &lt;= 1.58</text>\n",
       "<text text-anchor=\"middle\" x=\"346.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.583</text>\n",
       "<text text-anchor=\"middle\" x=\"346.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 87</text>\n",
       "<text text-anchor=\"middle\" x=\"346.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [43, 9, 35]</text>\n",
       "<text text-anchor=\"middle\" x=\"346.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = class_0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M297.7314,-222.8796C303.2373,-214.1434 309.1006,-204.8404 314.7824,-195.8253\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"317.7857,-197.6242 320.1567,-187.2981 311.8637,-193.8919 317.7857,-197.6242\"/>\n",
       "<text text-anchor=\"middle\" x=\"325.6863\" y=\"-207.9789\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#3de583\" stroke=\"#000000\" d=\"M111,-68C111,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 111,0 111,0 117,0 123,-6 123,-12 123,-12 123,-56 123,-56 123,-62 117,-68 111,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.036</text>\n",
       "<text text-anchor=\"middle\" x=\"61.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 54</text>\n",
       "<text text-anchor=\"middle\" x=\"61.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 53, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"61.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = class_1</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M146.231,-103.9815C134.6582,-94.4232 122.3611,-84.2668 110.8287,-74.7419\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.9243,-71.9332 102.9852,-68.2637 108.4666,-77.3304 112.9243,-71.9332\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M245.5,-68C245.5,-68 153.5,-68 153.5,-68 147.5,-68 141.5,-62 141.5,-56 141.5,-56 141.5,-12 141.5,-12 141.5,-6 147.5,0 153.5,0 153.5,0 245.5,0 245.5,0 251.5,0 257.5,-6 257.5,-12 257.5,-12 257.5,-56 257.5,-56 257.5,-62 251.5,-68 245.5,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = class_0</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M197.6171,-103.9815C197.8421,-95.618 198.0795,-86.7965 198.307,-78.3409\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"201.8078,-78.3542 198.5781,-68.2637 194.8103,-78.1659 201.8078,-78.3542\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#853fe6\" stroke=\"#000000\" d=\"M393,-68C393,-68 294,-68 294,-68 288,-68 282,-62 282,-56 282,-56 282,-12 282,-12 282,-6 288,0 294,0 294,0 393,0 393,0 399,0 405,-6 405,-12 405,-12 405,-56 405,-56 405,-62 399,-68 393,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"343.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.054</text>\n",
       "<text text-anchor=\"middle\" x=\"343.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 36</text>\n",
       "<text text-anchor=\"middle\" x=\"343.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1, 35]</text>\n",
       "<text text-anchor=\"middle\" x=\"343.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = class_2</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M345.3829,-103.9815C345.1579,-95.618 344.9205,-86.7965 344.693,-78.3409\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"348.1897,-78.1659 344.4219,-68.2637 341.1922,-78.3542 348.1897,-78.1659\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#ea985e\" stroke=\"#000000\" d=\"M534,-68C534,-68 435,-68 435,-68 429,-68 423,-62 423,-56 423,-56 423,-12 423,-12 423,-6 429,0 435,0 435,0 534,0 534,0 540,0 546,-6 546,-12 546,-12 546,-56 546,-56 546,-62 540,-68 534,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"484.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.265</text>\n",
       "<text text-anchor=\"middle\" x=\"484.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 51</text>\n",
       "<text text-anchor=\"middle\" x=\"484.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [43, 8, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"484.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = class_0</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M397.8861,-103.9815C409.7161,-94.4232 422.2864,-84.2668 434.0751,-74.7419\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"436.5142,-77.2708 442.0929,-68.2637 432.1149,-71.826 436.5142,-77.2708\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f4d59a1ada0>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(classifier, out_file=None, feature_names=wine.feature_names, \n",
    "                           class_names=wine.target_names, filled=True,\n",
    "                           rounded=True, special_characters=False)\n",
    "graph = graphviz.Source(dot_data)\n",
    "#graph.render(\"wine\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWsj9RxF_EQA"
   },
   "source": [
    "What rules can you find in the trees you have induced? Do they make sense?\n",
    "\n",
    "**Report:**\n",
    "1. How accurate are the decision tree classifiers for the two data sets? Look at overall accuracy as well as precision and recall for specific classes.\n",
    "2. How does training error relate to test error?\n",
    "3. Can you make sense of the rules implicit in the trees? Consider especially the pruned tree for the English past tense data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjMATs49Sfpp"
   },
   "source": [
    "## K-Nearest Neighbor\n",
    "\n",
    "The third task is to use k-nearest neighbor classification to predict the plural form of a German noun and the past tense form of an English verb. The classifier can be used as follows.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "AxmcBs3gJ4Ri",
    "outputId": "016c6fcb-ec1f-4917-e3e1-82be1121dbf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.33333333, 0.9       , 0.8       ]),\n",
       " array([0.5       , 0.81818182, 0.8       ]),\n",
       " array([0.4       , 0.85714286, 0.8       ]),\n",
       " array([ 2, 11,  5]))"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.9)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, knn.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lt3XV9kTIgE0"
   },
   "source": [
    "Compare training error to test error. Vary the number of neighbors used to predict the class and see how this affects training and test error. One of the properties of (simple) nearest neighbor classification is that all features are given equal weight, which means that irrelevant features could hurt classification accuracy. Check whether you can improve accuracy by removing features or reweighting samples. Compare the best accuracy to that obtained with decision trees.\n",
    "\n",
    "**Report:**\n",
    "1. How accurate are the nearest neighbor classifiers for the two data sets? Look at overall accuracy as well as precision and recall for specific classes.\n",
    "2. What is the effect of varying the k parameter? Plot the cross validated accuracy vs increasing k.\n",
    "3. Can you improve accuracy by removing less informative features?\n",
    "4. Does k-nearest neighbor perform better or worse than decision trees? Can you force the nearest neighbor classifier to behave like the pruned decision tree on the English past tense data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utGge3LFSWVd"
   },
   "source": [
    "**Report:** Which features are most informative for the two data sets? Try to explain why some features are more informative than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YdrPyjBuSps1"
   },
   "source": [
    "## VG assignment\n",
    "\n",
    "The above tasks are sufficient to obtain a pass grade (G) in this assignment. To obtain a pass with distinction (VG), all the assignments must be carried out without major errors, and there is an additional task you should solve.\n",
    "\n",
    "It is fairly obvious that the performance of a machine learning task depends on the amount of training data that is available, but the amount of data required to reach a certain level of performance in a specific task varies. Your last task is to relate the size of the training data to the performance you can achieve on the two data sets.\n",
    "\n",
    "Train your classifiers using the best hyperparameters you found for both prediction tasks on a varying amount of training data ranging from just a few examples to the full data set and test the resulting classifiers for each training set size. Do this for both types of classifiers (decision trees\n",
    "and kNN). Check if you can find a set of hyperparameters that work better for smaller training set size and compare the learning curves. Take a look at the rules in the decision trees for various training set sizes.\n",
    "\n",
    "**Report:** Describe how you ran these experiments and present the resulting learning curves, both in tabular and in graphical form. Report and discuss any interesting observations you made.\n",
    "\n",
    "# Submission\n",
    "\n",
    "You should submit one of the following two options:\n",
    "1. A written report (3-5 pages) in academic english, reporting and rguing for your conclusions on all tasks. All code must be included in an appentix and be well commented. The report should be submitted through Studentportalen.\n",
    "**OR**\n",
    "2. Your cleaned notebook with the relevant code and answers to all the questions. The ipynb-file should be submitted through Studentportalen. The code blocks should reproduce your main findings and be commented either in a text cell or standard code comments.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML lab 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
