{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "The code below can be used to download UD corpuses and save them in the work place.  \n",
    "\n",
    "Due to the limitation of the computer resource, test conllu files are used to illustrate the pipe line of preprocessing, training and evaluation. Thus, the overal everage accuray will be lower than the train conllu data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download UD corpus for Swedish, English, Finnish, German, Spanish\n",
      "Beginning file download with requests\n",
      "Assignment4_tmp.ipynb\n",
      "Assignment_4_Yongchao_Wu.ipynb\n",
      "ML_Assignment_4_RNN_based_POS_tagger_v1.ipynb\n",
      "Pytorch_Practice.ipynb\n",
      "de_pud-ud-test.conllu\n",
      "en_pud-ud-test.conllu\n",
      "es_pud-ud-test.conllu\n",
      "fi_pud-ud-test.conllu\n",
      "sv_pud-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "import requests\n",
    "print('Download UD corpus for Swedish, English, Finnish, German, Spanish')\n",
    "print('Beginning file download with requests')\n",
    "\n",
    "data_urls = ['https://raw.githubusercontent.com/UniversalDependencies/UD_Swedish-PUD/master/sv_pud-ud-test.conllu', \n",
    "             'https://raw.githubusercontent.com/UniversalDependencies/UD_English-PUD/master/en_pud-ud-test.conllu',\n",
    "             'https://raw.githubusercontent.com/UniversalDependencies/UD_Finnish-PUD/master/fi_pud-ud-test.conllu',\n",
    "             'https://raw.githubusercontent.com/UniversalDependencies/UD_German-PUD/master/de_pud-ud-test.conllu',\n",
    "             'https://raw.githubusercontent.com/UniversalDependencies/UD_Spanish-PUD/master/es_pud-ud-test.conllu']\n",
    "\n",
    "filenames = [url.split(\"/\")[-1] for url in data_urls]\n",
    "\n",
    "for url, fn in zip(data_urls, filenames):\n",
    "  urllib.request.urlretrieve(url, fn)\n",
    "\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class that can process and parse UD coll data\n",
    "The code below will parse UD coll data, extract tokens and relative POS. The code also split the dataset into training and testing for development and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For user feedback\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports for pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "import nlpaug.augmenter.char as nac\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "class CollProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def load_and_preprocess_conllu(self, file, augment=False):\n",
    "        X, Y = [], []\n",
    "        with open(file, \"r\") as infile:\n",
    "            sents = infile.read().split(\"\\n\\n\")\n",
    "            if sents[-1] == \"\":\n",
    "                sents = sents[:-1]\n",
    "            for sent in sents:\n",
    "                words, tags = [], []\n",
    "                lines = sent.split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    if line.startswith(\"#\"):\n",
    "                        continue\n",
    "                    line = line.strip().split(\"\\t\")\n",
    "                    if len(line) >=3:\n",
    "                        words.append(line[1])\n",
    "                        tags.append(line[3]) \n",
    "                X.append(words)\n",
    "                Y.append(tags)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "        # here user can turn on the data augmentation\n",
    "        if augment:\n",
    "            aug = nac.OcrAug()\n",
    "            #for sent in X_train:\n",
    "                #sent = [aug.augment(x) if x.isalpha() else x for x in sent]\n",
    "            X_train = [[aug.augment(x) if x.isalpha() else x for x in sent] for sent in X_train]\n",
    "    \n",
    "        word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        tag2idx = dict()\n",
    "    \n",
    "        for sentence, tags in zip(X_train, y_train):\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if word not in word2idx:    # The 'in' operator is fast for dictionaries\n",
    "                    word2idx[word] = len(word2idx)\n",
    "            for tag in tags:\n",
    "                if tag not in tag2idx:\n",
    "                    tag2idx[tag] = len(tag2idx)\n",
    "        tag2idx['<PAD>'] = len(tag2idx)\n",
    "        \n",
    "        idx2word = [None]*len(word2idx)\n",
    "        for word, idx in word2idx.items():\n",
    "            idx2word[idx] = word\n",
    "        idx2tag = [None]*len(tag2idx)\n",
    "        for tag, idx in tag2idx.items():\n",
    "            idx2tag[idx] = tag\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, word2idx, tag2idx, idx2tag, idx2word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Class that can pad and batch data\n",
    "The code below will pad the data in order to fit into pytorch.  \n",
    "The the DataLoader is used to batch pytorch data. So all the batched data has the same sentence length in pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPadder:\n",
    "    def __init__(self, sentences, labels, word2idx, tag2idx):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        \n",
    "    def pad_data(self):\n",
    "        sentences = self.sentences\n",
    "        labels = self.labels\n",
    "        max_len = np.max([len(sentence) for sentence in sentences]) # Find out how much to pad\n",
    "        padded_sentences  = list()\n",
    "        padded_labels  = list()\n",
    "        for sentence, tags in zip(sentences, labels):               # Loop over the data\n",
    "            padded_sentence = [word.lower() if word.lower() in self.word2idx else '<UNK>' for word in sentence]  # Sentence, uses the <UNK> symbol for unknown words\n",
    "            padded_tags = [tag for tag in tags]                       # Tags\n",
    "            while len(padded_sentence) < max_len:                     # Adds padding to the sequences \n",
    "                padded_sentence.append('<PAD>')\n",
    "                padded_tags.append('<PAD>')\n",
    "            padded_sentences.append(padded_sentence)                  # Append the processed sample to the output\n",
    "            padded_labels.append(padded_tags)\n",
    "        return padded_sentences, padded_labels\n",
    "\n",
    "    def encode_data(self):\n",
    "        padded_sentences, padded_labels = self.pad_data()\n",
    "        tag2idx = self.tag2idx\n",
    "        word2idx = self.word2idx\n",
    "        encoded_sentences = list()\n",
    "        for padded_sentence in padded_sentences:\n",
    "            encoded_sentences.append([word2idx[word] for word in padded_sentence])\n",
    "        encoded_labels = list()\n",
    "        for padded_tags in padded_labels:\n",
    "            encoded_labels.append([tag2idx[tag] for tag in padded_tags])\n",
    "        return torch.LongTensor(encoded_sentences), torch.LongTensor(encoded_labels)\n",
    "\n",
    "# here we use the Dataset and DataLoader to make the data looped by pythorch easier in the future. \n",
    "    def batch_data(self, batch_size):\n",
    "        features, labels = self.encode_data()\n",
    "        dataset = Data.TensorDataset(features, labels)\n",
    "        data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "        return data_iter\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagger Class\n",
    "\n",
    "The tagger class contains LSTM/GRU model related hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports for pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Tagger(nn.Module):\n",
    "  def __init__(self, model, word_embedding_dim, hidden_dim, vocabulary_size, tagset_size, dropout, bidirectional, word2idx):\n",
    "    \"\"\"\n",
    "    \n",
    "    word_embedding_dim\n",
    "      The dimensionality of the word embedding\n",
    "    hidden_dim\n",
    "      The dimensionality of the ...\n",
    "    vocabulary_size\n",
    "      The size of the vocabulary (i.e. the number of unique words in the word embedding)\n",
    "    tagset_size\n",
    "    \"\"\"\n",
    "    super(Tagger, self).__init__() # We need to initialise the class we are inheriting from\n",
    "    self.model = model\n",
    "    self.hidden_dim = hidden_dim                             # This simply stores the parameter\n",
    "    self.vocabulary_size = vocabulary_size\n",
    "    self.tagset_size = tagset_size\n",
    "    self.dropout = dropout\n",
    "    self.bidirectional = bidirectional\n",
    "    self.word2idx = word2idx\n",
    "    self._word_embeddings = nn.Embedding(num_embeddings=vocabulary_size,     # Creates the vector space for the input words\n",
    "                                         embedding_dim=word_embedding_dim, \n",
    "                                         padding_idx=self.word2idx['<PAD>'])\n",
    "    if model == \"LSTM\":\n",
    "        self._lstm = nn.LSTM(input_size=word_embedding_dim,                 # The Tagger takes an embedded sentence as input, and outputs \n",
    "                              hidden_size=hidden_dim,                   # vectors with dimensionality hidden_dim.\n",
    "                              batch_first=True,\n",
    "                              bidirectional=bidirectional, \n",
    "                              dropout=dropout)\n",
    "    elif model == \"GRU\":\n",
    "        self._gru = nn.GRU(input_size=word_embedding_dim,                 # The Tagger takes an embedded sentence as input, and outputs \n",
    "                              hidden_size=hidden_dim,                   # vectors with dimensionality hidden_dim.\n",
    "                              batch_first=True,\n",
    "                              bidirectional=bidirectional, \n",
    "                              dropout=dropout)\n",
    "\n",
    "    \n",
    "    self._hidden2tags = nn.Linear(hidden_dim, tagset_size)         # The linear layer maps from the RNN output space to tag space\n",
    "                                      \n",
    "  def forward(self, padded_sentences):\n",
    "    \"\"\"The forward pass through the network\"\"\"\n",
    "\n",
    "    batch_size, max_sentence_length = padded_sentences.size()\n",
    "    \n",
    "    embedded_sentences = self._word_embeddings(padded_sentences) \n",
    "\n",
    "    X = embedded_sentences\n",
    "\n",
    "    tag_space = self._hidden2tags(X)\n",
    "\n",
    "    # The output from the LSTM/GRU layer is flattened and passed to the fully \n",
    "                                                                        # connected layer.\n",
    "    tag_scores = F.log_softmax(tag_space, dim=1)                        # Softmax is applied to normalise the outputs\n",
    "\n",
    "    return tag_scores.view(batch_size, max_sentence_length, self.tagset_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe Class. \n",
    "This class is mainly used for tuning and carry different test. The pipe class itself is quite small and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipe:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.word2idx = None\n",
    "        self.tag2idx = None\n",
    "        self.idx2tag = None\n",
    "        self.idx2word = None\n",
    "        \n",
    "        self.data_iter = None\n",
    "        self.data_iter_test = None \n",
    "        \n",
    "        self.tagger = None\n",
    "        \n",
    "    def get_word2idx(self):\n",
    "        return self.word2idx\n",
    "        \n",
    "    def pre_process(self, file, augment=False):\n",
    "        \n",
    "        coll_processor = CollProcessor()\n",
    "        self.X_train, self.X_test, self.y_train, \\\n",
    "        self.y_test, self.word2idx, self.tag2idx, \\\n",
    "        self.idx2tag, self.idx2word \\\n",
    "        = coll_processor.load_and_preprocess_conllu(file, augment)\n",
    "        print(\"Using corpus: \", file)\n",
    "        print(\"There are overal\", len(self.X_train)+len(self.X_test), \"sentences in the corpus.\")\n",
    "        \n",
    "    def pad_and_batch(self, batch_size ):\n",
    "        data_padder = DataPadder(self.X_train, self.y_train, self.word2idx, self.tag2idx)\n",
    "        self.data_iter = data_padder.batch_data(batch_size = batch_size)\n",
    "        data_padder2 = DataPadder(self.X_test, self.y_test, self.word2idx, self.tag2idx)\n",
    "        self.data_iter_test = data_padder2.encode_data()\n",
    "        \n",
    "    def create_tagger(self, model, WORD_EMBEDDING_DIM, HIDDEN_DIM, dropout, bidirectional, word2idx, printout=False):\n",
    "        self.tagger = Tagger(model=model,word_embedding_dim=WORD_EMBEDDING_DIM, \n",
    "                        hidden_dim=HIDDEN_DIM,\n",
    "                        vocabulary_size=len(self.word2idx),                            # The vocabulary incudes both the 'padding' and 'unknown' symbols\n",
    "                        tagset_size=len(self.tag2idx)-1, \n",
    "                        dropout=dropout,\n",
    "                        bidirectional=bidirectional,\n",
    "                        word2idx=word2idx)\n",
    "        if printout:\n",
    "            print(\"The tagger is based on \",model, \", its network details can be summarized as below:\")\n",
    "            print(self.tagger)\n",
    "        \n",
    "    def train_the_tagger(self, learning_rate, weight_decay, epoch_num, plotout=False):\n",
    "        loss_function = nn.NLLLoss(ignore_index=self.tag2idx['<PAD>'])\n",
    "        optimizer = optim.Adam(self.tagger.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        loss_for_plotting = list()\n",
    "        start_t = time.time()\n",
    "        for epoch in range(epoch_num):\n",
    "            for inputs, targets in self.data_iter: # Now for looping over the training data\n",
    "                self.tagger.zero_grad()                                       # Reset gradients\n",
    "                tag_scores = self.tagger(inputs)                              # Forward pass\n",
    "                tag_scores = tag_scores.view(-1, self.tagger.tagset_size)     # Resize to get tag probabilities along a separate dimension\n",
    "                                                                    # but flatten all the different sentences\n",
    "                targets = targets.view(-1)                              # Flatten y\n",
    "                loss = loss_function(tag_scores, targets)               # Get loss\n",
    "                loss.backward()                                         # Backpropagate the error\n",
    "                optimizer.step()                                        # Run the optimizer to change the weights w.r.t the loss\n",
    "                loss_for_plotting.append(loss.item())                   # Save the loss for plotting\n",
    "        print(\"Epoch %i: loss=%.3f, taining time=%.1fs\" % (epoch_num, loss, time.time()-start_t))\n",
    "        \n",
    "        if plotout:\n",
    "            plt.figure()\n",
    "            plt.title(\"Plot for the (hopefully) decreasing loss over epochs\")\n",
    "            plt.plot(loss_for_plotting, '.-')\n",
    "            plt.xlabel(\"Batch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.show()\n",
    "        \n",
    "    def tag_mean_accuracy(self, y, y_hat):\n",
    "        accuracy = list()\n",
    "        for u, v in zip(y, y_hat):\n",
    "            accuracy.append(np.sum([e1==e2 for e1, e2 in zip(u, v)])/len(u))\n",
    "        return np.mean(accuracy), np.std(accuracy)\n",
    "    \n",
    "    def evaluate_tagger(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs, _ = self.data_iter_test                   # The test data must be in the allowed input format\n",
    "            tag_scores = self.tagger(inputs)                                        # Runs the test data through the model\n",
    "            tag_idx = np.asarray(np.argmax(tag_scores.numpy(), axis=2), dtype=np.int) # Get the most likely tag indeces\n",
    "            y_hat = list()                                                    # The model predicts indeces but we want tags\n",
    "            for i in range(tag_scores.shape[0]):                              # Loop over test samples\n",
    "                padding_mask = inputs[i, :] != self.word2idx['<PAD>']                # Prepare to mask out padding\n",
    "                y_hat.append([self.idx2tag[e] for e in tag_idx[i, padding_mask]])    # Remove padding and transform to tags\n",
    "\n",
    "        accuracy, sigma = self.tag_mean_accuracy(self.y_test, y_hat)\n",
    "        print(\"Mean accuracy: %.1f%% [std %.1f%%]\" % (100*accuracy, 100*sigma))\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small demo for pipe usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "The tagger is based on  GRU , its network details can be summarized as below:\n",
      "Tagger(\n",
      "  (_word_embeddings): Embedding(5045, 64, padding_idx=0)\n",
      "  (_gru): GRU(64, 64, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (_hidden2tags): Linear(in_features=64, out_features=17, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yonwu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: loss=1.281, taining time=3.3s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 73.7% [std 9.9%]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters are placed together, thus it is easier to tune the model.\n",
    "batch_size = 250\n",
    "WORD_EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "dropout = 0.1\n",
    "bidirectional = True\n",
    "model = \"GRU\"\n",
    "learning_rate=0.01\n",
    "weight_decay = 0.0001\n",
    "epoch_num = 30\n",
    "\n",
    "# The pipe class usage demo\n",
    "newpipe = pipe()\n",
    "newpipe.pre_process(file='en_pud-ud-test.conllu')\n",
    "word2idx = newpipe.get_word2idx()\n",
    "newpipe.pad_and_batch(batch_size=batch_size)\n",
    "\n",
    "newpipe.create_tagger(WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, HIDDEN_DIM=HIDDEN_DIM, dropout=dropout, bidirectional=bidirectional, model=model, word2idx = word2idx, printout=True)\n",
    "newpipe.train_the_tagger(learning_rate = learning_rate, weight_decay = weight_decay, epoch_num = epoch_num, plotout=True)\n",
    "newpipe.evaluate_tagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment A test to study relation between model complexity and accuracy. Using English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "The tagger is based on  GRU , its network details can be summarized as below:\n",
      "Tagger(\n",
      "  (_word_embeddings): Embedding(5045, 16, padding_idx=0)\n",
      "  (_gru): GRU(16, 16, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (_hidden2tags): Linear(in_features=16, out_features=17, bias=True)\n",
      ")\n",
      "Epoch 30: loss=1.559, taining time=3.4s\n",
      "Mean accuracy: 69.8% [std 10.7%]\n",
      "\n",
      "\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "The tagger is based on  GRU , its network details can be summarized as below:\n",
      "Tagger(\n",
      "  (_word_embeddings): Embedding(5045, 32, padding_idx=0)\n",
      "  (_gru): GRU(32, 32, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (_hidden2tags): Linear(in_features=32, out_features=17, bias=True)\n",
      ")\n",
      "Epoch 30: loss=1.412, taining time=3.6s\n",
      "Mean accuracy: 72.7% [std 9.8%]\n",
      "\n",
      "\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "The tagger is based on  GRU , its network details can be summarized as below:\n",
      "Tagger(\n",
      "  (_word_embeddings): Embedding(5045, 64, padding_idx=0)\n",
      "  (_gru): GRU(64, 64, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (_hidden2tags): Linear(in_features=64, out_features=17, bias=True)\n",
      ")\n",
      "Epoch 30: loss=1.297, taining time=4.0s\n",
      "Mean accuracy: 75.1% [std 9.6%]\n",
      "\n",
      "\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "The tagger is based on  GRU , its network details can be summarized as below:\n",
      "Tagger(\n",
      "  (_word_embeddings): Embedding(5045, 128, padding_idx=0)\n",
      "  (_gru): GRU(128, 128, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (_hidden2tags): Linear(in_features=128, out_features=17, bias=True)\n",
      ")\n",
      "Epoch 30: loss=1.253, taining time=4.7s\n",
      "Mean accuracy: 75.0% [std 10.4%]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 250\n",
    "DIM = [(16, 16), (32, 32), (64, 64), (128, 128)]\n",
    "dropout = 0.1\n",
    "bidirectional = True\n",
    "model = \"GRU\"\n",
    "learning_rate=0.01\n",
    "weight_decay = 0.0001\n",
    "epoch_num = 30\n",
    "corpus = 'en_pud-ud-test.conllu'\n",
    "\n",
    "for WORD_EMBEDDING_DIM, HIDDEN_DIM in DIM:\n",
    "    newpipe = pipe()\n",
    "    newpipe.pre_process(file=corpus)\n",
    "    word2idx = newpipe.get_word2idx()\n",
    "    newpipe.pad_and_batch(batch_size=batch_size)\n",
    "\n",
    "    newpipe.create_tagger(WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, HIDDEN_DIM=HIDDEN_DIM, dropout=dropout, bidirectional=bidirectional, model=model, word2idx = word2idx, printout=True)\n",
    "    newpipe.train_the_tagger(learning_rate = learning_rate, weight_decay = weight_decay, epoch_num = epoch_num)\n",
    "    newpipe.evaluate_tagger()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: A test to study relation between model complexity and accuracy. Using English, Spannish, German, French, Finnish Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using corpus:  sv_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=1.186, taining time=2.7s\n",
      "Mean accuracy: 68.3% [std 11.5%]\n",
      "\n",
      "\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=1.273, taining time=3.2s\n",
      "Mean accuracy: 73.1% [std 10.5%]\n",
      "\n",
      "\n",
      "Using corpus:  fi_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=1.255, taining time=2.6s\n",
      "Mean accuracy: 59.5% [std 11.8%]\n",
      "\n",
      "\n",
      "Using corpus:  de_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=1.328, taining time=3.4s\n",
      "Mean accuracy: 68.9% [std 10.8%]\n",
      "\n",
      "\n",
      "Using corpus:  es_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=1.470, taining time=3.7s\n",
      "Mean accuracy: 79.1% [std 10.3%]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for corpus in filenames:\n",
    "    newpipe = pipe()\n",
    "    newpipe.pre_process(file=corpus)\n",
    "    word2idx = newpipe.get_word2idx()\n",
    "    newpipe.pad_and_batch(batch_size=batch_size)\n",
    "\n",
    "    newpipe.create_tagger(WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, HIDDEN_DIM=HIDDEN_DIM, dropout=dropout, bidirectional=bidirectional, model=model, word2idx = word2idx)\n",
    "    newpipe.train_the_tagger(learning_rate = learning_rate, weight_decay = weight_decay, epoch_num = epoch_num)\n",
    "    newpipe.evaluate_tagger()\n",
    "    \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Trial of data augmentation using `nlpaug`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment4_tmp.ipynb\r\n",
      "Assignment_4_Yongchao_Wu.ipynb\r\n",
      "ML_Assignment_4_RNN_based_POS_tagger_v1.ipynb\r\n",
      "Pytorch_Practice.ipynb\r\n",
      "de_pud-ud-test.conllu\r\n",
      "en_pud-ud-test-paug.conllu\r\n",
      "en_pud-ud-test.conllu\r\n",
      "es_pud-ud-test.conllu\r\n",
      "fi_pud-ud-test.conllu\r\n",
      "sv_pud-ud-test.conllu\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cp en_pud-ud-test.conllu en_pud-ud-test-paug.conllu\n",
    "!ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do2ens', '0f', 'F8LN', 'fol1uwers', 'ake', 'a1keady', 'ce1e6ratin9', 'the', 'vict0ky', 'in', 'the', 'Pla2a', 'de', '1as', 'Victokia8', '.']\n",
      "['Dozens', 'of', 'FSLN', 'followers', 'are', 'already', 'celebrating', 'the', 'victory', 'in', 'the', 'Plaza', 'de', 'las', 'Victorias', '.']\n"
     ]
    }
   ],
   "source": [
    "conllu_p = CollProcessor()\n",
    "X_train_t, X_test_t, y_train_t, y_test_t, word2idx_T, tag2idx_t, idx2tag_t, idx2word_t = conllu_p.load_and_preprocess_conllu(file='en_pud-ud-test-paug.conllu', augment=True)\n",
    "print (X_train_t[1])\n",
    "conllu_p = CollProcessor()\n",
    "X_train_t, X_test_t, y_train_t, y_test_t, word2idx_T, tag2idx_t, idx2tag_t, idx2word_t = conllu_p.load_and_preprocess_conllu(file='en_pud-ud-test-paug.conllu', augment=False)\n",
    "print (X_train_t[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "The tagger is based on  GRU , its network details can be summarized as below:\n",
      "Tagger(\n",
      "  (_word_embeddings): Embedding(6309, 64, padding_idx=0)\n",
      "  (_gru): GRU(64, 64, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (_hidden2tags): Linear(in_features=64, out_features=17, bias=True)\n",
      ")\n",
      "Epoch 30: loss=1.322, taining time=3.5s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 49.3% [std 11.7%]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters are placed together, thus it is easier to tune the model.\n",
    "batch_size = 250\n",
    "WORD_EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "dropout = 0.1\n",
    "bidirectional = True\n",
    "model = \"GRU\"\n",
    "learning_rate=0.01\n",
    "weight_decay = 0.0001\n",
    "epoch_num = 30\n",
    "\n",
    "# The pipe class usage demo\n",
    "newpipe = pipe()\n",
    "newpipe.pre_process(file='en_pud-ud-test.conllu', augment=True)\n",
    "word2idx = newpipe.get_word2idx()\n",
    "newpipe.pad_and_batch(batch_size=batch_size)\n",
    "\n",
    "newpipe.create_tagger(WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, HIDDEN_DIM=HIDDEN_DIM, dropout=dropout, bidirectional=bidirectional, model=model, word2idx = word2idx, printout=True)\n",
    "newpipe.train_the_tagger(learning_rate = learning_rate, weight_decay = weight_decay, epoch_num = epoch_num, plotout=True)\n",
    "newpipe.evaluate_tagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop out value 0\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=1.301, taining time=3.4s\n",
      "Mean accuracy: 72.0% [std 9.9%]\n",
      "\n",
      "\n",
      "drop out value 0.1\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=1.315, taining time=3.4s\n",
      "Mean accuracy: 74.6% [std 10.3%]\n",
      "\n",
      "\n",
      "drop out value 0.5\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=1.265, taining time=3.4s\n",
      "Mean accuracy: 73.5% [std 10.1%]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters are placed together, thus it is easier to tune the model.\n",
    "batch_size = 250\n",
    "WORD_EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "dropout = [0, 0.1, 0.5]\n",
    "bidirectional = True\n",
    "model = \"GRU\"\n",
    "learning_rate=0.01\n",
    "weight_decay = 0.0001\n",
    "epoch_num = 30\n",
    "\n",
    "for dropout_value in dropout:\n",
    "    print(\"drop out value\", dropout_value)\n",
    "\n",
    "    newpipe = pipe()\n",
    "    newpipe.pre_process(file='en_pud-ud-test.conllu')\n",
    "    word2idx = newpipe.get_word2idx()\n",
    "    newpipe.pad_and_batch(batch_size=batch_size)\n",
    "\n",
    "    newpipe.create_tagger(WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, HIDDEN_DIM=HIDDEN_DIM, dropout=dropout_value, bidirectional=bidirectional, model=model, word2idx = word2idx, printout=False)\n",
    "    newpipe.train_the_tagger(learning_rate = learning_rate, weight_decay = weight_decay, epoch_num = epoch_num, plotout=False)\n",
    "    newpipe.evaluate_tagger()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight decay 0\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yonwu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: loss=1.060, taining time=3.1s\n",
      "Mean accuracy: 69.4% [std 10.0%]\n",
      "\n",
      "\n",
      "weight decay 0.0001\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=1.310, taining time=3.1s\n",
      "Mean accuracy: 75.6% [std 10.1%]\n",
      "\n",
      "\n",
      "weight decay 0.001\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=2.245, taining time=3.3s\n",
      "Mean accuracy: 57.5% [std 11.1%]\n",
      "\n",
      "\n",
      "weight decay 0.01\n",
      "Using corpus:  en_pud-ud-test.conllu\n",
      "There are overal 1000 sentences in the corpus.\n",
      "Epoch 30: loss=2.842, taining time=3.4s\n",
      "Mean accuracy: 47.5% [std 8.8%]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters are placed together, thus it is easier to tune the model.\n",
    "batch_size = 250\n",
    "WORD_EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "dropout = 0.1\n",
    "bidirectional = True\n",
    "model = \"GRU\"\n",
    "learning_rate=0.01\n",
    "weight_decay = [0, 0.0001, 0.001, 0.01]\n",
    "epoch_num = 30\n",
    "\n",
    "for wd_value in weight_decay:\n",
    "    print(\"weight decay\", wd_value)\n",
    "\n",
    "    newpipe = pipe()\n",
    "    newpipe.pre_process(file='en_pud-ud-test.conllu')\n",
    "    word2idx = newpipe.get_word2idx()\n",
    "    newpipe.pad_and_batch(batch_size=batch_size)\n",
    "\n",
    "    newpipe.create_tagger(WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, HIDDEN_DIM=HIDDEN_DIM, dropout=dropout, bidirectional=bidirectional, model=model, word2idx = word2idx, printout=False)\n",
    "    newpipe.train_the_tagger(learning_rate = learning_rate, weight_decay = wd_value, epoch_num = epoch_num, plotout=False)\n",
    "    newpipe.evaluate_tagger()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "1. A working implementation of your pipeline, reproducing your principal results when executed.\n",
    "\n",
    "  Done.\n",
    "  \n",
    "\n",
    "2. Comments in the code for longer or opaque functionality. Please try to write self documenting code (e.g. by choosing descriptive variables names etc).\n",
    "\n",
    "  Done.  \n",
    "\n",
    "3. A *brief* description of the implementation work that was necessary to complete the different parts, showing how you arrived at your solution.\n",
    "\n",
    "  Done.  \n",
    "  \n",
    "  First I implement classes that can handle different tasks(preprocessing, padding, batching, tagger model), then create a pipe class that includes the small classes. The advantage is that, this way will create a small and clean size of pipe line class. All hyperparameters are held in one place which will help us to tune the model easily. \n",
    "\n",
    "4. The given model refactored as a class (including relevant preprocessing, forward/backward pass etc).\n",
    "\n",
    "  Done.\n",
    "  \n",
    "  Same explaination as point 3. A short demo of pipe is showed in the above code cell. \n",
    "\n",
    "5. Not all languages require the same model complexity for POS tagging. Some might need a higher dimensionality for the embedding or layers for the model to perform well. What is the relation between model complexity and accuracy? Include a quantitative evaluation of data in five languages from the [universal dependencies](https://universaldependencies.org/) project (UD). These will be on the familiar conll format. Parse the files to get the words (not lemmas) and universal tags.\n",
    "\n",
    "  Done.\n",
    "  \n",
    "  For this part, 5 language UD Corpuses are used in the analysis(Spannish, English, German, Finnish, and Frech.)\n",
    "We can see that, with model complexity dimension increasing, at first the accuracy is increasing but then dropped a little bit. That means with a \"proper\" complexity dimension model, the accuracy can be improved. But if the complexity is too large, the model itself will not be trained properly, thus, the accuracy will not be improved.   \n",
    "\n",
    "  And we can see that, with same settings of model complexity, different language behaves differently. The result shows that Finnish performs a poor tagging accuracy because of a more rich language mophology nature. While for spannish and English, the accracy is higher.   \n",
    "\n",
    "  Note, because UD coll corpus only contains 1000 sentences. The overal result is not that high.   \n",
    "\n",
    "7. Comments on what you thought was hard in the assignment, what you think was educational, what took most time and which parts might be unnecessary tricky.\n",
    "\n",
    "  Done.   \n",
    "\n",
    "  I like this assignment. I learned a lot from the implementation of LSTM and GRU using pytorch and find pytorch is very powerful and convenient. I also learned a lot from refactoring the code from distributed functions into classes, which makes me think a lot about how to organize the machine learning code, how to arrange the hyperparameters to make the tuning easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions\n",
    " \n",
    "To get a passing grade (G), you should (in addition to the above stated points) extend the given model in at least three of the following ways. For a pass with distinction grade (VG), you should extend your analysis to include all of the following points. State clearly which ones you have done.\n",
    " \n",
    "1. There are other types of RNNs layers commonly used in NLP. Add the option to use a GRU layer instead of LSTM, and include this in your performance comparison.\n",
    "\n",
    "    Done.     \n",
    "    The code details is in class `Tagger`.  The `Tagger` has both options for LSTM and GRU.  \n",
    "    <br>\n",
    "2. The given implementation only allows for dependencies from left to right. Add the option to use a bi-directional pass in the RNN layer.\n",
    "\n",
    "    Done.     \n",
    "    The code details is in class `Tagger`.  bi-directional parameter is added.  \n",
    "    <br>  \n",
    "3. Use pytorch's `Dataset` and `DataLoader` classes for loading the data. [This tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) is a good starting point. This should make it easier to loop over data sets.\n",
    "    \n",
    "    Done.  \n",
    "    The code details is in class `DataPadder`.   `Dataset` and `DataLoader` are used to easily batch and loop data.  \n",
    "    <br>    \n",
    "4. Try some type of data augmentation in your training data. This should theoretically increase the generalisability of your model. See if this is true for your chosen data sources. How much augmentation is too much?\n",
    "\n",
    "    Done.  \n",
    "    NLP data augmentation library `nlpaug` is used to do the data augmentation. Simple change word spellings. For  example, after apply the data augmentation, the sentence looks like below:\n",
    "    `['Do2ens', '0f', 'F8LN', 'fol1uwers', 'ake', 'a1keady', 'ce1e6ratin9', 'the', 'vict0ky', 'in', 'the', 'Pla2a', 'de', '1as', 'Victokia8', '.']`\n",
    "     `['Dozens', 'of', 'FSLN', 'followers', 'are', 'already', 'celebrating', 'the', 'victory', 'in', 'the', 'Plaza', 'de', 'las', 'Victorias', '.'] `\n",
    "     But the result shows a lower accuray, which means the augmentation is too much. because there are too many unkown words after the augmentation and the tagger just makes wrong prediction. \n",
    "    More details could be referred to the code cell `Trial of data augmentation using nlpaug`. How to use turn on the data augmentation function should be referred to the class `CollProcessor`, there is a option `augment`. \n",
    "    \n",
    "    <br>    \n",
    "5. Implement some level of dropout in your model. See if dropout has any effect on the generalisability of your model. This can be implemeted in several ways, *briefly* argue or the choices you made.\n",
    "    \n",
    "    Done.  \n",
    "    In this work, dropout is just easily used the embedded pytorch dropout in lstm/gnu.  \n",
    "    According to the pytorch document. dropout here introduces a Dropout layer on the outputs of each LSTM/GNU layer except the last layer, with dropout probability equal to dropout. From the Experiment Cell we can see that, due to the limiation of the dataset, the overal accuracy is not good. While with a small dropout value, the accuracy imporved a bit. But if the dropout value is too big, the accuracy will be lower. \n",
    "    \n",
    "    <br>    \n",
    "6. Add more sources. You can either add more sources from one language but with different types of text or simply more languages. An interesting thing to try could be how languages with very different levels of morphology (e.g. english vs finish) requires more or less training data. The total number of sources should be above 15.\n",
    "\n",
    "    Done.  \n",
    "    Similar Analysis can be found in the code cell `Experiment: A test to study relation between model complexity and accuracy. Using English, Spannish, German, French, Finnish Corpus`. We can see that The result shows that Finnish performs a poor tagging accuracy because of a more rich language mophology nature. While for spannish and English, the accracy is higher. Thus, to reach same level of accuracy for language like Finnish, more complex model or more training data is needed. \n",
    "    If we want to add more language resources, just add the url in the file downloading code block:\n",
    "    ```\n",
    "  data_urls = ['https://raw.githubusercontent.com/UniversalDependencies/UD_Swedish-PUD/master/sv_pud-udtest.conllu', \n",
    "             'https://raw.githubusercontent.com/UniversalDependencies/UD_English-PUD/master/en_pud-ud-test.conllu',\n",
    "             'https://raw.githubusercontent.com/UniversalDependencies/UD_Finnish-PUD/master/fi_pud-ud-test.conllu',\n",
    "             'https://raw.githubusercontent.com/UniversalDependencies/UD_German-PUD/master/de_pud-ud-test.conllu',\n",
    "             'https://raw.githubusercontent.com/UniversalDependencies/UD_Spanish-PUD/master/es_pud-ud-test.conllu']\n",
    "             'more files'\n",
    "             \n",
    "    ```\n",
    "    <br>    \n",
    "7. Comment on shortcomings and how to improve the model more in line with the state-of-the-art. References to the relevant literature (a good starting point is [NLP-progress](http://nlpprogress.com/).\n",
    "\n",
    "    Done.    \n",
    "    The process of encoding the input as a fixed-size state vector (hidden state) is actually a process of \"information lossy compression\". If the amount of information is larger, the loss of information caused by the process of converting vectors is greater. What the attention model needs to do is to encode the encoder into different Context according to each time step of the sequence. When decoding, combine each different Context to decode the output, so that the result obtained will be more accurate.\n",
    "    Ref:\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan NGomez, ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 59986008. Curran Associates, Inc., 2017.  \n",
    "    \n",
    "   <br>      \n",
    "8. Examine how weight decay is implemented in pytorch and if it improves your model.\n",
    "    \n",
    "   Done.  \n",
    "   Weight decay in pytorch is implemented in different optimizer. In this assignment, the optimizer is torch.optim.Adam. In this optimizer, weight decay is implemented as L2 regularization. The experiment of weight decay can be referred in the code bock `Experiment with weight decay.` With a proper `weight decay`, the model accuracy will be improved. \n",
    "    <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
